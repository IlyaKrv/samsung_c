# Проектирование и реализация конвейера обработки данных для анализа подкастов

**Требования**  
- Windows 10/11 / Linux / macOS  
- Docker + Docker Compose  
- Доступ к интернету  

---

##  Установка и настройка

### 1. Подготовка данных
Перед запуском ETL-конвейера необходимо обработать исходные аудио/сырые транскрипты с помощью **собственного парсера**, расположенного в папке `parser/`.

Результат работы парсера — текстовые файлы в формате:

[00:00 - 00:15] Привет! Сегодня поговорим о больших данных.
[00:16 - 00:30] Отличная тема! Я давно хотел об этом рассказать...


Поместите полученные `.txt` файлы в: data/raw/transcripts/


> ⚠️ Конвейер **ожидает именно такой формат**. Убедитесь, что парсер корректно генерирует временные метки и реплики.

### 2. Запуск через Docker
Проект полностью контейнеризован. Выполните в корне проекта:

bash
# Первый запуск (сборка + запуск)
docker-compose up --build

# Или перезапуск после изменений
docker-compose down -v
docker-compose up --build

###3. Доступ к сервисам
Дашборд: http://localhost:8501
HDFS Web UI: http://localhost:9870
MongoDB: доступна внутри контейнеров по адресу mongodb://mongodb:27017

CТРУКТУРА ПРОЕКТА

samsun_project/
├── parser/                    # ← Собственный парсер (обработка сырых данных → .txt)
├── data/
│   └── raw/transcripts/       # ← Готовые структурированные .txt файлы
├── src/
│   ├── process_podcasts.py    # ETL: HDFS → NLP-анализ → MongoDB + Parquet
│   └── streamlit_app.py       # Интерактивный дашборд
├── docker-compose.yml         # Оркестрация: HDFS, MongoDB, ETL-приложение
├── Dockerfile                 # Образ Python-приложения
├── hadoop.env                 # Конфигурация Hadoop
└── requirements.txt           # Зависимости (включая spaCy и ru_core_news_sm)

Авторы
Илья Кривошеев
Алексей Бусарев
Курсовой проект по дисциплине «Big Data»
