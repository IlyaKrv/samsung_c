import os
import re
import time
import logging
from datetime import datetime
import numpy as np
from collections import Counter
from pymongo import MongoClient
from hdfs import InsecureClient
import pandas as pd


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ===
HDFS_URL = os.getenv("HDFS_URL", "http://namenode:9870")
MONGODB_URL = os.getenv("MONGODB_URL", "mongodb://mongodb:27017")
LOCAL_DATA_DIR = "/app/data/raw/transcripts"
HDFS_RAW_PATH = "/podcasts/raw"


def wait_for_hdfs(max_wait=120):
    """–û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ HDFS"""
    logger.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ HDFS (–º–∞–∫—Å. {max_wait} —Å–µ–∫)...")

    start_time = time.time()
    while time.time() - start_time < max_wait:
        try:
            client = InsecureClient(HDFS_URL, user='root', timeout=10)
            status = client.status('/')
            logger.info("‚úÖ HDFS –≥–æ—Ç–æ–≤!")
            return client
        except Exception as e:
            logger.debug(f"–û–∂–∏–¥–∞–Ω–∏–µ HDFS... ({e})")
            time.sleep(5)




def upload_local_files_to_hdfs():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ HDFS"""
    client = wait_for_hdfs()

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ HDFS
    try:
        client.makedirs(HDFS_RAW_PATH)
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è HDFS: {HDFS_RAW_PATH}")
    except:
        logger.info(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {HDFS_RAW_PATH}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã
    if not os.path.exists(LOCAL_DATA_DIR):
        logger.error(f" –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {LOCAL_DATA_DIR}")
        return

    files = [f for f in os.listdir(LOCAL_DATA_DIR) if f.endswith('.txt')]
    logger.info(f" –ù–∞–π–¥–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(files)}")

    for filename in files:
        local_path = os.path.join(LOCAL_DATA_DIR, filename)
        hdfs_path = f"{HDFS_RAW_PATH}/{filename}"

        try:
            with open(local_path, 'rb') as f:
                client.write(hdfs_path, f, overwrite=True)
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –≤ HDFS: {filename}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")



def time_to_seconds(time_str):
    try:
        m, s = map(int, time_str.split(':'))
        return m * 60 + s
    except:
        return 0


def parse_transcript(text):
    pattern = r'\[(\d{2}:\d{2}) - (\d{2}:\d{2})\]\s*(.+?)(?=\[\d{2}:\d{2}|$)'
    dialogues = []

    if not text or not text.strip():
        return dialogues

    for match in re.finditer(pattern, text, re.MULTILINE | re.DOTALL):
        try:
            start_time, end_time, speaker_text = match.groups()
            duration = time_to_seconds(end_time) - time_to_seconds(start_time)

            if duration <= 0:
                continue

            dialogues.append({
                'start_time': start_time,
                'end_time': end_time,
                'duration': duration,
                'text': speaker_text.strip(),
                'words_count': len(speaker_text.split())
            })
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∏–∞–ª–æ–≥–∞: {e}")
            continue

    return dialogues


def simple_sentiment(text):
    """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è"""
    if not text:
        return 0

    text_lower = text.lower()
    positive = len(re.findall(r'—Ö–æ—Ä–æ—à|–∫—Ä—É—Ç–æ|–∫–ª–∞—Å—Å|–æ—Ç–ª–∏—á–Ω–æ|—Å–ø–∞—Å–∏–±–æ|—Å—É–ø–µ—Ä|–ø—Ä–µ–∫—Ä–∞—Å–Ω|–ª—é–±–ª—é|–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω', text_lower))
    negative = len(re.findall(r'–ø–ª–æ—Ö|—É–∂–∞—Å–Ω|–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω|–Ω–µ–Ω–∞–≤–∏–∂—É|—Ä–∞–∑–¥—Ä–∞–∂–∞–µ—Ç|—Å–∫—É—á–Ω', text_lower))
    return positive - negative


def extract_keywords(texts, top_n=5):
    if not texts:
        return []

    stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–Ω–µ', '—á—Ç–æ', '—ç—Ç–æ', '–∫–∞–∫', '–∞', '–Ω–æ', '–¥–∞', '—Ç—ã', '—è', '–µ—â—ë', '—Ç–∞–º',
                  '–≤–æ—Ç', '—Ç–∞–∫', '–∂–µ', '–Ω—É', '–∏–ª–∏'}
    all_words = []

    for text in texts:
        words = re.findall(r'\b\w{3,}\b', text.lower())
        all_words.extend([w for w in words if w not in stop_words and len(w) > 2])

    word_counts = Counter(all_words)
    return [word for word, count in word_counts.most_common(top_n)]


def analyze_speakers(dialogues):
    if not dialogues:
        return {"–°–ø–∏–∫–µ—Ä1": 0, "–°–ø–∏–∫–µ—Ä2": 0}

    speakers = Counter()
    for dialog in dialogues:
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ø–∏–∫–µ—Ä–∞
        first_words = dialog['text'][:30].lower()
        if any(w in first_words for w in ['—è', '–º–Ω–µ', '–º–æ–π', '–º–æ–µ']):
            speaker = "–°–ø–∏–∫–µ—Ä1"
        elif any(w in first_words for w in ['—Ç—ã', '–≤—ã', '–≤–∞—à']):
            speaker = "–°–ø–∏–∫–µ—Ä2"
        else:
            speaker = "–°–ø–∏–∫–µ—Ä1"  # default

        speakers[speaker] += dialog['words_count']

    return dict(speakers)


def process_episode(filename, text):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞"""
    dialogues = parse_transcript(text)

    if not dialogues:
        logger.warning(f" –ù–µ—Ç –¥–∏–∞–ª–æ–≥–æ–≤ –≤ —Ñ–∞–π–ª–µ: {filename}")
        return None

    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    texts = [d['text'] for d in dialogues]
    total_duration = sum(d['duration'] for d in dialogues)
    total_words = sum(d['words_count'] for d in dialogues)

    words_per_minute = total_words / (total_duration / 60) if total_duration > 0 else 0

    # ML-–∞–Ω–∞–ª–∏–∑
    keywords = extract_keywords(texts, top_n=3)
    speakers_words = analyze_speakers(dialogues)
    sentiments = [simple_sentiment(d['text']) for d in dialogues]
    avg_sentiment = np.mean(sentiments) if sentiments else 0

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
    record = {
        "episode_id": os.path.splitext(filename)[0],
        "filename": filename,
        "dialogues_count": len(dialogues),
        "total_duration_sec": int(total_duration),
        "total_duration_min": round(total_duration / 60, 1),
        "total_words": total_words,
        "words_per_minute": round(words_per_minute, 1),
        "avg_dialogue_duration": round(np.mean([d['duration'] for d in dialogues]), 1),
        "avg_sentiment": round(avg_sentiment, 2),
        "keywords": keywords,
        "topics": ", ".join(keywords[:3]),  # –î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        "speaker1_words": speakers_words.get("–°–ø–∏–∫–µ—Ä1", 0),
        "speaker2_words": speakers_words.get("–°–ø–∏–∫–µ—Ä2", 0),
        "speaker_balance": round(speakers_words.get("–°–ø–∏–∫–µ—Ä1", 0) / max(total_words, 1), 2),
        "processed_at": datetime.now().isoformat(),
        "raw_dialogues": dialogues[:10]  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ 10 –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    }

    return record


def process_episodes_from_hdfs():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –∏–∑ HDFS"""
    records = []

    try:
        client = wait_for_hdfs()

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–∑ HDFS
        try:
            files = client.list(HDFS_RAW_PATH)
            logger.info(f" –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –≤ HDFS: {len(files)}")
        except Exception as e:
            logger.error(f" –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è HDFS: {e}")
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –∫–∞–∫ fallback
            return process_episodes_local()

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
        for filename in files:
            if not filename.endswith('.txt'):
                continue

            logger.info(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞: {filename}")

            try:
                # –ß—Ç–µ–Ω–∏–µ –∏–∑ HDFS
                with client.read(f"{HDFS_RAW_PATH}/{filename}") as reader:
                    content = reader.read()
                    text = content.decode('utf-8')

                record = process_episode(filename, text)
                if record:
                    records.append(record)
                    logger.info(f"‚úÖ {filename}: {record['dialogues_count']} –¥–∏–∞–ª–æ–≥–æ–≤, "
                                f"{record['words_per_minute']:.1f} —Å–ª–æ–≤/–º–∏–Ω")

            except UnicodeDecodeError:
                # –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É
                try:
                    with client.read(f"{HDFS_RAW_PATH}/{filename}") as reader:
                        content = reader.read()
                        text = content.decode('cp1251', errors='replace')
                    record = process_episode(filename, text)
                    if record:
                        records.append(record)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ {filename}: {e}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {filename}: {e}")

    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ HDFS: {e}")
        records = process_episodes_local()  # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã

    return records


def process_episodes_local():
    """Fallback: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    records = []

    if not os.path.exists(LOCAL_DATA_DIR):
        logger.error(f"‚ùå –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {LOCAL_DATA_DIR}")
        return records

    files = [f for f in os.listdir(LOCAL_DATA_DIR) if f.endswith('.txt')]
    logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã: {len(files)}")

    for filename in files:
        filepath = os.path.join(LOCAL_DATA_DIR, filename)

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                text = f.read()
        except UnicodeDecodeError:
            try:
                with open(filepath, 'r', encoding='cp1251') as f:
                    text = f.read()
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filename}: {e}")
                continue

        record = process_episode(filename, text)
        if record:
            records.append(record)

    return records


def save_to_mongodb(records):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ MongoDB"""
    if not records:
        logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ MongoDB")
        return

    try:
        client = MongoClient(MONGODB_URL, serverSelectionTimeoutMS=5000)
        client.admin.command('ping')  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è

        db = client["podcasts_db"]
        coll = db["podcasts_ml"]

        # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
        coll.delete_many({})

        # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        result = coll.insert_many(records)

        logger.info(f"‚úÖ MongoDB: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(result.inserted_ids)} –∑–∞–ø–∏—Å–µ–π")
        client.close()

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ MongoDB: {e}")
        # –°–æ–∑–¥–∞–µ–º fallback - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        import json
        with open('/app/data/fallback_data.json', 'w') as f:
            json.dump(records, f, indent=2, default=str)
        logger.info("üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ fallback JSON —Ñ–∞–π–ª")


def save_to_parquet(records):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Parquet –¥–ª—è Streamlit"""
    if not records:
        logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Parquet")
        return

    try:
        df = pd.DataFrame(records)

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        columns_to_drop = ['_id', 'processed_at', 'raw_dialogues']
        for col in columns_to_drop:
            if col in df.columns:
                df = df.drop(columns=[col])

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        parquet_dir = "/app/data/parquet"
        os.makedirs(parquet_dir, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        parquet_path = os.path.join(parquet_dir, "podcasts_ml.parquet")
        df.to_parquet(parquet_path, index=False)

        logger.info(f"‚úÖ Parquet —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {parquet_path} ({len(df)} –∑–∞–ø–∏—Å–µ–π)")
        logger.info(f"üìä –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Parquet: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–æ–π ETL –ø—Ä–æ—Ü–µ—Å—Å"""
    logger.info("=" * 50)
    logger.info("üöÄ –ó–ê–ü–£–°–ö ML-ETL –ü–†–û–¶–ï–°–°–ê")
    logger.info("=" * 50)

    start_time = time.time()

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã –≤ HDFS
    logger.info("üì§ –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ HDFS...")
    upload_local_files_to_hdfs()

    # 2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ HDFS
    logger.info("üîß –®–∞–≥ 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ HDFS...")
    records = process_episodes_from_hdfs()

    if not records:
        logger.error("‚ùå –ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã –≤ data/raw/transcripts/")
        return

    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ MongoDB
    logger.info("üíæ –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ MongoDB...")
    save_to_mongodb(records)

    # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet
    logger.info("üíæ –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Parquet...")
    save_to_parquet(records)

    # 5. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
    total_time = time.time() - start_time
    logger.info("=" * 50)
    logger.info(f"üéâ ETL –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
    logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
    logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —ç–ø–∏–∑–æ–¥–æ–≤: {len(records)}")
    logger.info("=" * 50)


if __name__ == "__main__":
    main()